{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Recommender Systems or Why Your Phone Isn't Actually Spying on You**\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1vpzhvPNf6fMdW1eettSzHHFeggNUzRZB\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Indaba_2023_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Authors:** Amrit Purshotam, Jama Mohamud\n",
        "\n",
        "**Reviewers:** Kyle Taylor\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Recommender Systems are probably one of the most ubiquitous type of machine learning model that we encounter in our online life. They influence what we see in our social media feeds, the products we buy, the music we listen to, the food we eat, and the movies we watch. Sometimes they're so good that people feel that their phone is spying on their conversations! In this prac, we hope to convince you that this isn't the case (mostly) as well as taking you through some of the techniques popularly used in industry that recommends the content you see online by building our very own movie recommender system.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: Machine Learning, Recommender Systems, Approximate Nearest Neighbours\n",
        "\n",
        "Level: <font color='grey'>`Beginner`</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- General architecture of a recommender systems.\n",
        "- Techniques for making recommendations.\n",
        "- Serving recommendations efficiently in production.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ],
      "metadata": {
        "id": "952qogb79nnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"advanced\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "if experience == \"beginner\":\n",
        "  sections_to_follow=\"Introduction -> 1.1 Subsection -> 2.1 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"intermediate\":\n",
        "  sections_to_follow=\"Introduction -> 1.2 Subsection -> 2.2 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"advanced\":\n",
        "  sections_to_follow=\"Introduction -> 1.3 Subsection -> 2.3 Subsection -> Conclusion -> Feedback\"\n",
        "\n",
        "print(f\"Based on your experience, it is advised you follow these -- {sections_to_follow} sections. Note this is just a guideline.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YBdDHcI_ArCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from flax import linen as nn\n",
        "from jax import numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "from flax import struct\n",
        "from clu import metrics\n",
        "from flax.training import train_state\n",
        "\n",
        "from typing import Iterable, Mapping, Sequence, Tuple\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper methods\n",
        "\n",
        "def get_dataset(ds_name: str) -> pd.DataFrame:\n",
        "  ds, info = tfds.load(f'movielens/{ds_name}', data_dir=\"./data\", with_info=True)\n",
        "  df = tfds.as_dataframe(ds['train'], info)\n",
        "  df = df.astype({'user_id': int, 'movie_id': int})\n",
        "  df.loc[:, 'movie_title'] = df['movie_title'].str.decode(\"utf-8\")\n",
        "  return df\n",
        "\n",
        "def cross_tabulate(df: pd.DataFrame, num_samples: int = 10) -> pd.DataFrame:\n",
        "  pivot_df = df.pivot(index='user_id', columns='movie_id', values='user_rating')\n",
        "  pivot_df.loc[df['user_id'].sample(num_samples), df['movie_id'].sample(num_samples)].dropna(axis=0, thresh=1).fillna(\"\")\n",
        "  return pivot_df\n",
        "\n",
        "def make_mapping(id_set: Iterable[str]) -> Mapping[str, int]:\n",
        "  return {id_str: i for (i, id_str) in enumerate(id_set)}\n",
        "\n",
        "def densify_column_values(df: pd.DataFrame, col_name: str) -> Tuple[pd.Series, Sequence[str]]:\n",
        "  col_values = sorted(set(df[col_name]))\n",
        "  col_ids_map = make_mapping(col_values)\n",
        "  return df[col_name].apply(lambda col_id: col_ids_map[col_id]), col_values, col_ids_map\n",
        "\n",
        "def to_tfds(df: pd.DataFrame) -> tf.data.Dataset:\n",
        "  fields = {\n",
        "    ('user_id', tf.int32),\n",
        "    ('item_id', tf.int32),\n",
        "    ('user_rating', tf.float32),\n",
        "    ('timestamp', tf.int32)\n",
        "  }\n",
        "\n",
        "  tensor_slices = {\n",
        "      field: tf.cast(df[field].values, dtype=field_type)\n",
        "      for field, field_type in fields\n",
        "  }\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices(tensor_slices)"
      ],
      "metadata": {
        "id": "TeWbblqpNutK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    DATASET = \"latest-small-ratings\" # all the options are 100k-ratings 1m-ratings 20m-ratings 25m-ratings latest-small-ratings\n",
        "    SEED = 42\n",
        "    EMB_DIM = 50\n",
        "    LR = 5e-3\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 3\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "bKVQDEz_eQWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **A Real World Scenario**\n",
        "\n",
        "Imagine you're going shopping for a new book. You enter the store and start walking around scanning the shelves of books. Something catches your interest, you pause, take the book down and inspect the cover, and perhaps also read the blurb. You continue to do this until you find something you like at which point you purchase the book and leave.\n",
        "\n",
        "Now imagine you've read the book and quite enjoyed it and you want to buy another one. You go back to the store, browse around, and occasionally inspect some books that catch your interest. An assistant this time approaches you asking if you need help. Gladly you accept, and you mention the books you were looking at as well as the one that you purchased recently. Since they've been working there a long time and have helped many customers, they now have a good idea of what you may like and recommends a short list of books for you to look at. You do so and eventually settle on one to purchase.\n",
        "\n",
        "**Exercise**. Let's pause here for a moment and dig deeper into this scenario.\n",
        "\n",
        "- From all the books in the store, what does it say about the ones you paused to look at, the ones you ignored, and the ones you purchased?\n",
        "- What does it say about you as the reader and your preferences? Could there be\n",
        "other people like you?\n",
        "- How was the assistant able to narrow down all the books in the store to just a few from which you actually purchased one?\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1kG2UcOybxNTWCCI1E3IwH1R--yVqpi_u\" />\n",
        "\n",
        "Source: Kim Falk. *Practical Recommender Systems*. 2019. Manning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer to these questions is now getting into the heart of recommender systems. Your behaviour probably wasn't random but instead had some structure and logic to it. You also likely have particular preferences to some genres and/or authors. Without even knowing anything else about the books, this follows then that the books you looked at and purchased matched those preferences and the ones you ignored more likely did not. Additionally, based on your preferences, what was catching your interest, and what you previously read, the store assistant was able to stitch together a rough profile of you. She then thought about her previous customers similar to you and what they had previously purchased. This is how she was then able to shortlist relevant books for you to look at.\n",
        "\n",
        "As you probably guessed it by now, the store assistant is the recommender system in this example. Instead of a person though, we want to build something that is able to learn the latent structure between all the books, all our customers, and how well they match each other in order to then make our recommendations. In the following sections we will learn how to do this. But first let's go over the general architecture of a recommender system."
      ],
      "metadata": {
        "id": "Jm3yxTa9pp8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Recommendation System Architecture**\n",
        "\n",
        "### Overview\n",
        "![picture](https://drive.google.com/uc?id=1m7ARSm_4PAaN6qP969HeHIGUfOjHX3Y9)\n",
        "Source: [System Design Interview](https://medium.com/double-pointer/system-design-interview-recommendation-system-design-as-used-by-youtube-netflix-etc-c457aaec3ab)\n",
        "\n",
        "The above diagram generalises our previous scenario from books to searches, songs, and movies (note there will usually only be one) while the user is you as before. Notice how the user interacts with these *items* which then gets sent to the recommender system as feedback. The recommender system processes this, retrieves relevant items from it's database and then serves them to the user. The user, in turn, further interacts with these items and continues this loop until some terminating criteria. Perhaps they found a movie they like and started watching, or they purchase a book like in our previous example, or they just simply leave.\n",
        "\n",
        "### Zooming in\n",
        "\n",
        "Let's zoom in closer to the recommender system now by having a look at how YouTube described theirs back in 2016 in their seminal paper on the topic. Being YouTube, the items here would be videos which number in the millions (and most likely in the billions as of writing in 2023). Pay special attention to the blue stages, notice the number of items going into each stage going down and hence the funnel shape.\n",
        "\n",
        "Looking at the first one, the job of the *candidate generation* stage is to efficiently *retrieve* relevant items from your database (which is why you may find in the literature, this stage is also known as *retrieval*). Speed is of the utmost importance here so some leeway is allowed in terms of relevancy as long as the number of items we reduce down to is manageable for the downstream parts of the recommender. This lookup speed is achieved partly by the techniques we discuss and implement later in this practical but also by limiting the number of features that feed into this stage.\n",
        "\n",
        "The second *ranking* stage (also known as *scoring*) then takes these items and sorts them based on additional features that come from the user as well as the features of the item itself to optimise for some target we care about using machine learning. In the case of YouTube, it will be for watch time, or in the case of an e-commerce website, the likelihood to purchase the items. Another reason for a ranking stage is that you may have multiple candidate generators and you now need a way to combine the results in some optimal way, at which point they're then shown to the user. It's also important to note, this ranking stage is not always necessary. Sometimes the retrieval step is good enough and you can keep the complexity of the system down, reduce implementation times, lower maintenance overhead, and therefore costs.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1SmTqpqZFwrf4-3CmXp3vVQl8er6d4XAn)\n",
        "\n",
        "Source: [Deep Neural Networks for YouTube Recommendations]()\n",
        "\n",
        "Finally, this last diagram, courtesy of the recommendations team at NVIDIA, further expands on the above ideas by defining two more stages namely *filtering* and *ordering*. After the retrieval / candidate generation stage, we may find that some of the items, while relevant, aren't useful and so need to be filtered out. In an e-commerce scenario this could be an item that's out of stock or in the case of a social media platform, a post coming from a person or topic you've blocked / muted. The *ordering* step which takes place after ranking / scoring then refers to further refining the order of the items depending on some business logic. For example, promoting on sale items or perhaps even sponsored placements.\n",
        "![picture](https://drive.google.com/uc?id=12fWnK5lrtdUX79GXT3tzOp6Fr78mKh57)\n",
        "\n",
        "Source: [Recommender Systems, Not Just Recommender Models](https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e)\n",
        "\n",
        "The rest of this practical will now focus primarily on the candidate generation stage of a recommender system. We hope this introduction and overview of recommender systems helps put into context the specific piece we will be building out. In particular, we will be learning about Collabarative Filtering and Graph Neural Networks for recommendations. But first, let's explore the data."
      ],
      "metadata": {
        "id": "Ox8aHBxO4luZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The MovieLens Dataset**\n",
        "\n",
        "Since we don't have access to an actual streaming service watch history, we will instead use a dataset called [MovieLens](https://grouplens.org/datasets/movielens/). The full dataset contains 25 million ratings across 62 thousand movies, created by 162 thousand users. However, to keep processing and training times low, we will be using a subset of 100 thousand ratings from over 9 thousand movies and 600 users. Let's have a look."
      ],
      "metadata": {
        "id": "YiXBlHjrb0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_dataset(config.DATASET)\n",
        "df.head(5)[['user_id', 'movie_title', 'user_rating', 'timestamp']]"
      ],
      "metadata": {
        "id": "m9WIKY4GeLM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And what if we cross-tabulate a sample of this data to get an alternative view."
      ],
      "metadata": {
        "id": "n5DIRRBLhcK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_tabulate(df)"
      ],
      "metadata": {
        "id": "9NWEC-ZliR3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table displayed above shows some of the more popular movies and users. The empty cells are what we want our model to learn to fill in i.e. the movies we presume a user has not yet watched because they have yet to rate it. Then once we make these predictions, we can figure out which of those movies they're most likely to enjoy.\n",
        "\n",
        "## **Dataset Preparation**\n",
        "\n",
        "Now let's prepare our dataset for training. We will be mapping our users and movies from indexes starting from 0."
      ],
      "metadata": {
        "id": "Dj9F3b3Cictf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['user_id'], user_list, user_to_id_mapping = densify_column_values(df, 'user_id')\n",
        "df['item_id'], movie_list, movie_to_id_mapping = densify_column_values(df, 'movie_title')\n",
        "df.head(5)[['user_id', 'item_id', 'user_rating', 'timestamp']]"
      ],
      "metadata": {
        "id": "S2qg8FcoKt6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will split our data randomly into a train and validation set and create our dataloaders that will feed data into our training process later. The exact details here aren't important so don't worry if you don't fully understand the below code."
      ],
      "metadata": {
        "id": "vxuOf62RLQjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_df = df.sample(frac=0.2)\n",
        "train_df = df[~df.index.isin(val_df.index)]\n",
        "\n",
        "train_ds = (\n",
        "  to_tfds(train_df)\n",
        "  .repeat(config.NUM_EPOCHS)\n",
        "  .shuffle(1024)\n",
        "  .batch(config.BATCH_SIZE, drop_remainder=False)\n",
        "  .prefetch(1)\n",
        ")\n",
        "val_ds = (\n",
        "  to_tfds(val_df)\n",
        "  .shuffle(1024)\n",
        "  .batch(config.BATCH_SIZE, drop_remainder=False)\n",
        "  .prefetch(1)\n",
        ")"
      ],
      "metadata": {
        "id": "ekPnIBs0LPmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to learn and implement Collaborative Filtering."
      ],
      "metadata": {
        "id": "3ACVEHMFU-kx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Collaborative Filtering**\n",
        "\n",
        "Collaboratve Filtering uses similarities between users and items simultaneously to provide recommendations.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1qxPGR6bXjDbigYxvFhE-bbeUK8mS-1Ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Measures\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1cqEj6omOtiSjcVf9EKtlBQP59orNuiVa\" width=\"50%\" />\n",
        "\n",
        "### Tensorboard Projector"
      ],
      "metadata": {
        "id": "_W5tQW6oD_9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedback"
      ],
      "metadata": {
        "id": "CVun4_JjD5oK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lv0agB-SD8kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph Neural Networks**\n",
        "\n",
        "[Background/content for the section.]"
      ],
      "metadata": {
        "id": "ugXf0-FkqHBs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}